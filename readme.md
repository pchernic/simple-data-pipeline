# ğŸ“Š Simple Data Pipeline

This is a simple data pipeline project built with Python and Docker to demonstrate how to extract, transform, and load (ETL) data using a clean and reproducible environment.

It processes a medical dataset in CSV format by:

- Extracting the raw data
- Cleaning and transforming it (removing nulls and standardizing column names)
- Saving the cleaned data to a new CSV file

---

## ğŸ§° Tech Stack

- Python 3.10
- pandas
- Docker & Docker Compose

---

## ğŸ“ Project Structure

```
simple-data-pipeline/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ pipeline.py              # Main ETL script
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Medicaldataset.csv       # Raw medical data (input)
â”‚   â””â”€â”€ CleanedMedicalData.csv   # Cleaned data (output)
â”œâ”€â”€ Dockerfile                   # Docker image build definition
â”œâ”€â”€ docker-compose.yml           # Runs the pipeline as a container
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md
```

---

## âš™ï¸ How to Run

### ğŸ”¹ Option 1: Docker (recommended)

Make sure you have Docker installed. Then run:

```bash
docker compose up --build
```

This will:

- Build the container image
- Mount the `data/` folder
- Run the `pipeline.py` script inside the container

When completed, youâ€™ll see:

```bash
Data Extraction completed.
Data Transformation completed.
Data Loading completed.
Data pipeline completed successfully.
```

Check `data/CleanedMedicalData.csv` for the result.

---

### ğŸ”¸ Option 2: Run locally (Python only)

1. Create and activate a virtual environment (optional but recommended):

```bash
python -m venv venv
venv\Scripts\activate  # On Windows
```

2. Install the dependencies:

```bash
pip install -r requirements.txt
```

3. Run the pipeline:

```bash
python app/pipeline.py
```

---

## âœ… Sample Output (first rows)

```csv
age,gender,heart_rate,systolic_blood_pressure,diastolic_blood_pressure,blood_sugar,ck-mb,troponin,result
64,1,66,160,83,160,1.8,0.012,negative
21,1,94,98,46,296,6.75,1.06,positive
...
```

---

## ğŸ§  Ideas for Extensions

Here are a few ideas to evolve this project:

### ğŸ” 1. **Data Validation**

Use `pydantic` or `pandera` to validate input data types and ranges.

### ğŸ“Š 2. **Data Visualization**

Add a Jupyter notebook or a Streamlit dashboard to show:

- Distribution of results
- Correlations between features
- Outliers

### ğŸ“ 3. **Database Integration**

Instead of writing to a CSV, send the cleaned data to:

- PostgreSQL
- SQLite
- Google BigQuery (for cloud-based pipeline)

### ğŸ›† 4. **Scheduled Pipelines**

Use `Apache Airflow`, `Prefect` or a simple `cron job` to schedule regular runs.

### â˜ï¸ 5. **Cloud Deployment**

Containerize it on AWS ECS, Lambda (with Layers), or GCP Cloud Run.

---

[ğŸ“˜ Source: KDnuggets](https://www.kdnuggets.com/2020/01/top-20-data-science-pipeline.html)


## ğŸ‘¤ Author

**Paulo Chernicharo**\
[GitHub](https://github.com/pchernic)\
[LinkedIn](https://www.linkedin.com/in/pchernic/)

